{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e81f6a5",
   "metadata": {},
   "source": [
    "### Given a topic research the latest news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ba322a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from utils import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1502a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- \"A simple, somber room with a portrait of Silvio Berlusconi on the wall. The camera slowly zooms in on the portrait.\"',\n",
       " '- \"A montage of clips showcasing Berlusconi\\'s political career and business achievements. Images of newspaper headlines detailing his scandals are interspersed.\"',\n",
       " '- \"Footage of lowered flags across Italy and Europe. A crowd gathered for the funeral at Milan Cathedral.\"',\n",
       " '- \"A series of photos and clips of world leaders offering tributes.\"',\n",
       " '- \"Clips of Berlusconi\\'s center-right party, Forza Italia, and its activities under Giorgia Meloni\\'s leadership. Images of Berlusconi\\'s public appearances before his death.\"']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.SCRIPT = \"outputs/silvio/scriptwriter_out.txt\"\n",
    "scenes = utils.get_scenes()\n",
    "# Extract lines starting with \"[Scene\"\n",
    "scene_lines = [s.description for s in scenes]\n",
    "scene_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6c69e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\"SG161222/Realistic_Vision_V2.0\", torch_dtype=torch.float16)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d016f806",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline.to(\"cuda\")\n",
    "pipeline.enable_xformers_memory_efficient_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b58b568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[diffusers.schedulers.scheduling_ddim.DDIMScheduler,\n",
       " diffusers.schedulers.scheduling_lms_discrete.LMSDiscreteScheduler,\n",
       " diffusers.schedulers.scheduling_euler_discrete.EulerDiscreteScheduler,\n",
       " diffusers.schedulers.scheduling_heun_discrete.HeunDiscreteScheduler,\n",
       " diffusers.schedulers.scheduling_dpmsolver_multistep.DPMSolverMultistepScheduler,\n",
       " diffusers.schedulers.scheduling_unipc_multistep.UniPCMultistepScheduler,\n",
       " diffusers.schedulers.scheduling_euler_ancestral_discrete.EulerAncestralDiscreteScheduler,\n",
       " diffusers.schedulers.scheduling_k_dpm_2_ancestral_discrete.KDPM2AncestralDiscreteScheduler,\n",
       " diffusers.utils.dummy_torch_and_torchsde_objects.DPMSolverSDEScheduler,\n",
       " diffusers.schedulers.scheduling_pndm.PNDMScheduler,\n",
       " diffusers.schedulers.scheduling_ddpm.DDPMScheduler,\n",
       " diffusers.schedulers.scheduling_dpmsolver_singlestep.DPMSolverSinglestepScheduler,\n",
       " diffusers.schedulers.scheduling_deis_multistep.DEISMultistepScheduler,\n",
       " diffusers.schedulers.scheduling_k_dpm_2_discrete.KDPM2DiscreteScheduler]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.scheduler.compatibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b73d9bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DPMSolverMultistepScheduler\n",
    "\n",
    "pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07844b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_prompt = \"\"\"\n",
    "(high detailed skin:1.2), 8k uhd, dslr, soft lighting, (high quality), film grain, Fujifilm XT3, (reportage, journalism)\n",
    "\"\"\"\n",
    "\n",
    "negative_prompt=\"\"\"\n",
    "((text:1.4, deformed iris, deformed pupils, semi-realistic, cgi, 3d, render, sketch, cartoon, drawing, anime:1.4, nudity)),\n",
    "close up, cropped, out of frame, worst quality, low quality, jpeg artifacts, ugly, duplicate,\n",
    "morbid, mutilated, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation,\n",
    "deformed, blurry, burns, dehydrated, bad anatomy, bad proportions, extra limbs, cloned face, disfigured,\n",
    "gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, fused fingers,\n",
    "too many fingers, long neck\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78a44865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96724691bb74f62a60ea430668fd277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9055f28390844d08a66540281ade0954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6d9517c7b3948f7bc0ec9d28a4f7ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b55f0eff89a4a9c9c4db6df525334ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (79 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['journalism )']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c82de5147da480d9c584ed45781f01f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "guidance_scale        = 0.7\n",
    "num_inference_steps   = 50\n",
    "num_images_per_prompt = 5\n",
    "\n",
    "for i,line in enumerate(scene_lines):\n",
    "    prompt = line\n",
    "    images = pipeline(\n",
    "        prompt=\"{} {}\".format(prompt, positive_prompt),\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_images_per_prompt=num_images_per_prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        #guidance_scale=guidance_scale,\n",
    "    ).images\n",
    "        \n",
    "    for j,image in enumerate(images):\n",
    "        image.save(\"outputs/silvio/scene_{}_{}.png\".format(i+1, j+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54e964b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory for upscaler\n",
    "del pipeline\n",
    "del images\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35ef9617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c08c9c788b24ec5b2d329550cf4dd95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)p16/model_index.json:   0%|          | 0.00/485 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced601f4f7d74914879f2c17365877bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f2c6416f6b4e91b8736791eb3d20cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cheduler_config.json:   0%|          | 0.00/300 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69978d5a91fa411380e25073beb07b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3dda7b2282b4207a91af50dd048efd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/460 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b04cdea7d1f447b94c7020d2c4abb82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/825 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7aa471fed18423cb499a04cf5f5735d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_encoder/config.json:   0%|          | 0.00/634 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6124a24c69a043f49b5bee10d2cd23df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)60b/unet/config.json:   0%|          | 0.00/982 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba69e82347d4ad4958a3ab5b52d1683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cheduler_config.json:   0%|          | 0.00/348 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7faa57d63d4dff909608bc950ccc63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/681M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931e4e84cb6d48c092e346600ef00d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3143b57f870460597bcebc7da15fdf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)360b/vae/config.json:   0%|          | 0.00/558 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e92a1d3788049e19ed42968657e0229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)on_pytorch_model.bin:   0%|          | 0.00/111M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d91e69fb7d4890b8bd6503bd82f1be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)on_pytorch_model.bin:   0%|          | 0.00/947M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moe/.pyenv/versions/3.10.6/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py:120: FutureWarning: The configuration file of the vae does not contain `scaling_factor` or it is set to 0.18215, which seems highly unlikely. If your checkpoint is a fine-tuned version of `stabilityai/stable-diffusion-x4-upscaler` you should change 'scaling_factor' to 0.08333 Please make sure to update the config accordingly, as not doing so might lead to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull Request for the `vae/config.json` file\n",
      "  deprecate(\"wrong scaling_factor\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from diffusers import StableDiffusionUpscalePipeline\n",
    "\n",
    "import torch\n",
    "\n",
    "pipeline = StableDiffusionUpscalePipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-x4-upscaler\",\n",
    "    revision=\"fp16\",\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8bfedca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xformers.ops import MemoryEfficientAttentionFlashAttentionOp\n",
    "\n",
    "pipeline = pipeline.to(\"cuda\")\n",
    "pipeline.enable_xformers_memory_efficient_attention(attention_op=MemoryEfficientAttentionFlashAttentionOp)\n",
    "# Workaround for not accepting attention shape using VAE for Flash Attention\n",
    "pipeline.vae.enable_xformers_memory_efficient_attention(attention_op=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ce36e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9740f3f9fc40a9b03a436eaf05f8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae66f5f4526e4fd9ac55d144d7ce2a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b49c1543a014040a37205b0a8d22654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c9319f97794497974ae8d959a0d88a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (79 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['journalism )']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca06effd53244158b1030d1897c66d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "guidance_scale        = 0.7\n",
    "num_inference_steps   = 20\n",
    "num_images_per_prompt = 1\n",
    "\n",
    "for i,prompt in enumerate(scene_lines):\n",
    "    low_res_image = Image.open(\"outputs/silvio/scene_{}_2.png\".format(i+1)).convert(\"RGB\")\n",
    "    low_res_image = low_res_image.resize((256,256))\n",
    "\n",
    "    image = pipeline(\n",
    "        prompt=\"{} {}\".format(prompt, positive_prompt),\n",
    "        negative_prompt=negative_prompt,\n",
    "        image=low_res_image,\n",
    "        num_images_per_prompt=num_images_per_prompt,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        #guidance_scale=guidance_scale,    \n",
    "    ).images[0]\n",
    "\n",
    "    image.save(\"outputs/silvio/scene_{}_2_upscaled.png\".format(i+1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
